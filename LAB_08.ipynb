{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNEKcKIlFR1cBaPlWAcYPjG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ilaharshith/Reinforcement-Learning-/blob/main/LAB_08.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jnk53ISNRV9K",
        "outputId": "547810b7-c7ca-41dc-ff7b-4647cec85f80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 10\tAvgReward(50)=-1027.73\tLastReward=-1200.86\tPolicyLoss=29.9340\tValueLoss=619.6361\tEntropy=0.9358\n",
            "Ep 20\tAvgReward(50)=-1231.11\tLastReward=-1564.66\tPolicyLoss=278.6509\tValueLoss=31455.6992\tEntropy=0.9618\n",
            "Ep 30\tAvgReward(50)=-1334.03\tLastReward=-1462.08\tPolicyLoss=515.6377\tValueLoss=298174.7500\tEntropy=0.9853\n",
            "Ep 40\tAvgReward(50)=-1306.81\tLastReward=-1398.27\tPolicyLoss=684.8461\tValueLoss=277495.1875\tEntropy=0.9840\n",
            "Ep 50\tAvgReward(50)=-1309.60\tLastReward=-1345.48\tPolicyLoss=803.1285\tValueLoss=218649.7188\tEntropy=0.9761\n",
            "Ep 60\tAvgReward(50)=-1328.08\tLastReward=-1117.27\tPolicyLoss=714.1961\tValueLoss=696091.0000\tEntropy=0.9751\n",
            "Training finished in 25.61 seconds.\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import time\n",
        "import random\n",
        "from collections import namedtuple\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Gymnasium is the modern replacement for Gym\n",
        "try:\n",
        "    import gymnasium as gym\n",
        "except ImportError:\n",
        "    raise ImportError(\"Please install gymnasium: pip install gymnasium[classic-control]\")\n",
        "\n",
        "\n",
        "# -------------------- Actor Network --------------------\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, obs_dim, act_dim, hidden_sizes=(64, 64), log_std_init=-0.5):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        last = obs_dim\n",
        "        for h in hidden_sizes:\n",
        "            layers.append(nn.Linear(last, h))\n",
        "            layers.append(nn.ReLU())\n",
        "            last = h\n",
        "        self.net = nn.Sequential(*layers)\n",
        "        self.mean = nn.Linear(last, act_dim)\n",
        "        self.log_std = nn.Parameter(torch.ones(act_dim) * log_std_init)\n",
        "\n",
        "    def forward(self, obs):\n",
        "        x = self.net(obs)\n",
        "        mean = self.mean(x)\n",
        "        log_std = self.log_std.expand_as(mean)\n",
        "        std = torch.exp(log_std)\n",
        "        return mean, std, log_std\n",
        "\n",
        "\n",
        "# -------------------- Critic Network --------------------\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, obs_dim, hidden_sizes=(64, 64)):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        last = obs_dim\n",
        "        for h in hidden_sizes:\n",
        "            layers.append(nn.Linear(last, h))\n",
        "            layers.append(nn.ReLU())\n",
        "            last = h\n",
        "        layers.append(nn.Linear(last, 1))\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, obs):\n",
        "        return self.net(obs).squeeze(-1)\n",
        "\n",
        "\n",
        "# -------------------- Utilities --------------------\n",
        "Transition = namedtuple('Transition', ['obs', 'act', 'rew', 'next_obs', 'done', 'logp'])\n",
        "\n",
        "def gaussian_log_prob(mean, log_std, act):\n",
        "    var = torch.exp(2 * log_std)\n",
        "    logp = -0.5 * (((act - mean) ** 2) / var + 2 * log_std + math.log(2 * math.pi))\n",
        "    return logp.sum(axis=-1)\n",
        "\n",
        "def sample_action(mean, std):\n",
        "    eps = torch.randn_like(mean)\n",
        "    return mean + eps * std\n",
        "\n",
        "\n",
        "# -------------------- A2C Agent --------------------\n",
        "class A2CAgent:\n",
        "    def __init__(self, obs_dim, act_dim, device='cpu', actor_lr=3e-4, critic_lr=1e-3,\n",
        "                 gamma=0.99, value_coef=0.5, entropy_coef=1e-3, max_grad_norm=0.5):\n",
        "        self.device = device\n",
        "        self.actor = Actor(obs_dim, act_dim).to(device)\n",
        "        self.critic = Critic(obs_dim).to(device)\n",
        "        self.optimizer_actor = optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
        "        self.optimizer_critic = optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
        "        self.gamma = gamma\n",
        "        self.value_coef = value_coef\n",
        "        self.entropy_coef = entropy_coef\n",
        "        self.max_grad_norm = max_grad_norm\n",
        "\n",
        "    def get_action(self, obs):\n",
        "        obs_t = torch.as_tensor(obs, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "            mean, std, log_std = self.actor(obs_t)\n",
        "            act = sample_action(mean, std)\n",
        "            logp = gaussian_log_prob(mean, log_std, act)\n",
        "            value = self.critic(obs_t)\n",
        "        return act.cpu().numpy()[0], logp.cpu().numpy()[0], value.cpu().numpy()[0]\n",
        "\n",
        "    def update(self, transitions, last_value=0.0):\n",
        "        obs = torch.as_tensor(np.vstack([t.obs for t in transitions]), dtype=torch.float32, device=self.device)\n",
        "        acts = torch.as_tensor(np.vstack([t.act for t in transitions]), dtype=torch.float32, device=self.device)\n",
        "        rewards = [t.rew for t in transitions]\n",
        "        dones = [t.done for t in transitions]\n",
        "        old_logps = torch.as_tensor(np.array([t.logp for t in transitions]), dtype=torch.float32, device=self.device)\n",
        "\n",
        "        # Compute returns (bootstrapped)\n",
        "        returns = []\n",
        "        R = last_value\n",
        "        for r, done in zip(reversed(rewards), reversed(dones)):\n",
        "            R = r + self.gamma * R * (1.0 - float(done))\n",
        "            returns.insert(0, R)\n",
        "        returns = torch.as_tensor(returns, dtype=torch.float32, device=self.device)\n",
        "\n",
        "        # Value and policy losses\n",
        "        values = self.critic(obs)\n",
        "        mean, std, log_std = self.actor(obs)\n",
        "        new_logps = gaussian_log_prob(mean, log_std, acts)\n",
        "        entropy = (0.5 * (1.0 + math.log(2 * math.pi)) + log_std).sum(-1).mean()\n",
        "\n",
        "        advantages = returns - values.detach()\n",
        "        policy_loss = -(new_logps * advantages).mean()\n",
        "        value_loss = 0.5 * (returns - values).pow(2).mean()\n",
        "        actor_loss = policy_loss - self.entropy_coef * entropy\n",
        "\n",
        "        # Update actor\n",
        "        self.optimizer_actor.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        nn.utils.clip_grad_norm_(self.actor.parameters(), self.max_grad_norm)\n",
        "        self.optimizer_actor.step()\n",
        "\n",
        "        # Update critic\n",
        "        self.optimizer_critic.zero_grad()\n",
        "        value_loss.backward()\n",
        "        nn.utils.clip_grad_norm_(self.critic.parameters(), self.max_grad_norm)\n",
        "        self.optimizer_critic.step()\n",
        "\n",
        "        return dict(policy_loss=policy_loss.item(), value_loss=value_loss.item(), entropy=entropy.item())\n",
        "\n",
        "\n",
        "# -------------------- Training Loop --------------------\n",
        "def train(env_name='Pendulum-v1', seed=1, episodes=500, max_steps=200, n_steps=5, render=False):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    env = gym.make(env_name)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    obs_dim = env.observation_space.shape[0]\n",
        "    act_dim = env.action_space.shape[0]\n",
        "    agent = A2CAgent(obs_dim, act_dim, device=device)\n",
        "\n",
        "    total_rewards = []\n",
        "    for ep in range(1, episodes + 1):\n",
        "        obs, info = env.reset(seed=seed)\n",
        "        ep_reward = 0.0\n",
        "        done = False\n",
        "        step = 0\n",
        "        buffer = []\n",
        "\n",
        "        while not done and step < max_steps:\n",
        "            act, logp, value = agent.get_action(obs)\n",
        "            next_obs, rew, terminated, truncated, info = env.step(act)\n",
        "            done_flag = terminated or truncated\n",
        "\n",
        "            buffer.append(Transition(obs=obs, act=act, rew=rew, next_obs=next_obs, done=done_flag, logp=logp))\n",
        "            ep_reward += rew\n",
        "            obs = next_obs\n",
        "            step += 1\n",
        "\n",
        "            if len(buffer) >= n_steps or done_flag or step >= max_steps:\n",
        "                if done_flag:\n",
        "                    last_value = 0.0\n",
        "                else:\n",
        "                    obs_t = torch.as_tensor(obs, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "                    with torch.no_grad():\n",
        "                        last_value = agent.critic(obs_t).cpu().numpy()[0]\n",
        "                stats = agent.update(buffer, last_value=last_value)\n",
        "                buffer = []\n",
        "\n",
        "            if done_flag:\n",
        "                break\n",
        "\n",
        "        total_rewards.append(ep_reward)\n",
        "        if ep % 10 == 0:\n",
        "            avg = np.mean(total_rewards[-50:])\n",
        "            print(f\"Ep {ep}\\tAvgReward(50)={avg:.2f}\\tLastReward={ep_reward:.2f}\\t\"\n",
        "                  f\"PolicyLoss={stats['policy_loss']:.4f}\\tValueLoss={stats['value_loss']:.4f}\\tEntropy={stats['entropy']:.4f}\")\n",
        "\n",
        "    env.close()\n",
        "    return agent, total_rewards\n",
        "\n",
        "\n",
        "# -------------------- Entry Point --------------------\n",
        "if __name__ == \"__main__\":\n",
        "    start = time.time()\n",
        "    agent, rewards = train(episodes=60, max_steps=200, n_steps=5)\n",
        "    print('Training finished in', round(time.time() - start, 2), 'seconds.')\n"
      ]
    }
  ]
}