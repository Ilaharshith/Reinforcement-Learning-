{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNanMOB7lHpcGxPyhDO/PNi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ilaharshith/Reinforcement-Learning-/blob/main/Untitled16.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7JsAOZrt5QfX",
        "outputId": "b21b074a-cb34-46de-a02f-0f1ed9b1f599"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 27, Episode Reward: 27.0\n",
            "Step 40, Episode Reward: 13.0\n",
            "Step 56, Episode Reward: 16.0\n",
            "Step 74, Episode Reward: 18.0\n",
            "Step 114, Episode Reward: 40.0\n",
            "Step 127, Episode Reward: 13.0\n",
            "Step 147, Episode Reward: 20.0\n",
            "Step 199, Episode Reward: 52.0\n",
            "Step 211, Episode Reward: 12.0\n",
            "Step 227, Episode Reward: 16.0\n",
            "Step 255, Episode Reward: 28.0\n",
            "Step 265, Episode Reward: 10.0\n",
            "Step 277, Episode Reward: 12.0\n",
            "Step 287, Episode Reward: 10.0\n",
            "Step 306, Episode Reward: 19.0\n",
            "Step 342, Episode Reward: 36.0\n",
            "Step 376, Episode Reward: 34.0\n",
            "Step 391, Episode Reward: 15.0\n",
            "Step 411, Episode Reward: 20.0\n",
            "Step 424, Episode Reward: 13.0\n",
            "Step 444, Episode Reward: 20.0\n",
            "Step 462, Episode Reward: 18.0\n",
            "Step 489, Episode Reward: 27.0\n",
            "Step 517, Episode Reward: 28.0\n",
            "Step 534, Episode Reward: 17.0\n",
            "Step 565, Episode Reward: 31.0\n",
            "Step 581, Episode Reward: 16.0\n",
            "Step 594, Episode Reward: 13.0\n",
            "Step 606, Episode Reward: 12.0\n",
            "Step 637, Episode Reward: 31.0\n",
            "Step 655, Episode Reward: 18.0\n",
            "Step 671, Episode Reward: 16.0\n",
            "Step 704, Episode Reward: 33.0\n",
            "Step 721, Episode Reward: 17.0\n",
            "Step 733, Episode Reward: 12.0\n",
            "Step 748, Episode Reward: 15.0\n",
            "Step 758, Episode Reward: 10.0\n",
            "Step 771, Episode Reward: 13.0\n",
            "Step 803, Episode Reward: 32.0\n",
            "Step 823, Episode Reward: 20.0\n",
            "Step 853, Episode Reward: 30.0\n",
            "Step 881, Episode Reward: 28.0\n",
            "Step 894, Episode Reward: 13.0\n",
            "Step 911, Episode Reward: 17.0\n",
            "Step 924, Episode Reward: 13.0\n",
            "Step 937, Episode Reward: 13.0\n",
            "Step 954, Episode Reward: 17.0\n",
            "Step 963, Episode Reward: 9.0\n",
            "Step 990, Episode Reward: 27.0\n",
            "Step 1003, Episode Reward: 13.0\n",
            "Step 1027, Episode Reward: 24.0\n",
            "Step 1045, Episode Reward: 18.0\n",
            "Step 1056, Episode Reward: 11.0\n",
            "Step 1070, Episode Reward: 14.0\n",
            "Step 1092, Episode Reward: 22.0\n",
            "Step 1106, Episode Reward: 14.0\n",
            "Step 1129, Episode Reward: 23.0\n",
            "Step 1148, Episode Reward: 19.0\n",
            "Step 1173, Episode Reward: 25.0\n",
            "Step 1185, Episode Reward: 12.0\n",
            "Step 1203, Episode Reward: 18.0\n",
            "Step 1221, Episode Reward: 18.0\n",
            "Step 1237, Episode Reward: 16.0\n",
            "Step 1249, Episode Reward: 12.0\n",
            "Step 1262, Episode Reward: 13.0\n",
            "Step 1295, Episode Reward: 33.0\n",
            "Step 1329, Episode Reward: 34.0\n",
            "Step 1364, Episode Reward: 35.0\n",
            "Step 1374, Episode Reward: 10.0\n",
            "Step 1385, Episode Reward: 11.0\n",
            "Step 1408, Episode Reward: 23.0\n",
            "Step 1430, Episode Reward: 22.0\n",
            "Step 1453, Episode Reward: 23.0\n",
            "Step 1465, Episode Reward: 12.0\n",
            "Step 1484, Episode Reward: 19.0\n",
            "Step 1499, Episode Reward: 15.0\n",
            "Step 1522, Episode Reward: 23.0\n",
            "Step 1542, Episode Reward: 20.0\n",
            "Step 1585, Episode Reward: 43.0\n",
            "Step 1598, Episode Reward: 13.0\n",
            "Step 1613, Episode Reward: 15.0\n",
            "Step 1626, Episode Reward: 13.0\n",
            "Step 1641, Episode Reward: 15.0\n",
            "Step 1671, Episode Reward: 30.0\n",
            "Step 1685, Episode Reward: 14.0\n",
            "Step 1701, Episode Reward: 16.0\n",
            "Step 1716, Episode Reward: 15.0\n",
            "Step 1738, Episode Reward: 22.0\n",
            "Step 1751, Episode Reward: 13.0\n",
            "Step 1778, Episode Reward: 27.0\n",
            "Step 1791, Episode Reward: 13.0\n",
            "Step 1820, Episode Reward: 29.0\n",
            "Step 1832, Episode Reward: 12.0\n",
            "Step 1845, Episode Reward: 13.0\n",
            "Step 1868, Episode Reward: 23.0\n",
            "Step 1884, Episode Reward: 16.0\n",
            "Step 1895, Episode Reward: 11.0\n",
            "Step 1906, Episode Reward: 11.0\n",
            "Step 1920, Episode Reward: 14.0\n",
            "Step 1934, Episode Reward: 14.0\n",
            "Step 1950, Episode Reward: 16.0\n",
            "Step 1975, Episode Reward: 25.0\n",
            "Step 1988, Episode Reward: 13.0\n",
            "Step 1999, Episode Reward: 11.0\n",
            "Step 2032, Episode Reward: 33.0\n",
            "Step 2049, Episode Reward: 17.0\n",
            "Step 2076, Episode Reward: 27.0\n",
            "Step 2090, Episode Reward: 14.0\n",
            "Step 2102, Episode Reward: 12.0\n",
            "Step 2136, Episode Reward: 34.0\n",
            "Step 2148, Episode Reward: 12.0\n",
            "Step 2159, Episode Reward: 11.0\n",
            "Step 2173, Episode Reward: 14.0\n",
            "Step 2185, Episode Reward: 12.0\n",
            "Step 2201, Episode Reward: 16.0\n",
            "Step 2216, Episode Reward: 15.0\n",
            "Step 2277, Episode Reward: 61.0\n",
            "Step 2295, Episode Reward: 18.0\n",
            "Step 2312, Episode Reward: 17.0\n",
            "Step 2322, Episode Reward: 10.0\n",
            "Step 2333, Episode Reward: 11.0\n",
            "Step 2351, Episode Reward: 18.0\n",
            "Step 2371, Episode Reward: 20.0\n",
            "Step 2383, Episode Reward: 12.0\n",
            "Step 2402, Episode Reward: 19.0\n",
            "Step 2419, Episode Reward: 17.0\n",
            "Step 2429, Episode Reward: 10.0\n",
            "Step 2465, Episode Reward: 36.0\n",
            "Step 2474, Episode Reward: 9.0\n",
            "Step 2507, Episode Reward: 33.0\n",
            "Step 2523, Episode Reward: 16.0\n",
            "Step 2568, Episode Reward: 45.0\n",
            "Step 2579, Episode Reward: 11.0\n",
            "Step 2595, Episode Reward: 16.0\n",
            "Step 2617, Episode Reward: 22.0\n",
            "Step 2632, Episode Reward: 15.0\n",
            "Step 2644, Episode Reward: 12.0\n",
            "Step 2660, Episode Reward: 16.0\n",
            "Step 2676, Episode Reward: 16.0\n",
            "Step 2686, Episode Reward: 10.0\n",
            "Step 2712, Episode Reward: 26.0\n",
            "Step 2735, Episode Reward: 23.0\n",
            "Step 2747, Episode Reward: 12.0\n",
            "Step 2776, Episode Reward: 29.0\n",
            "Step 2795, Episode Reward: 19.0\n",
            "Step 2822, Episode Reward: 27.0\n",
            "Step 2832, Episode Reward: 10.0\n",
            "Step 2843, Episode Reward: 11.0\n",
            "Step 2858, Episode Reward: 15.0\n",
            "Step 2869, Episode Reward: 11.0\n",
            "Step 2880, Episode Reward: 11.0\n",
            "Step 2896, Episode Reward: 16.0\n",
            "Step 2912, Episode Reward: 16.0\n",
            "Step 2926, Episode Reward: 14.0\n",
            "Step 2951, Episode Reward: 25.0\n",
            "Step 2962, Episode Reward: 11.0\n",
            "Step 2981, Episode Reward: 19.0\n",
            "Step 2999, Episode Reward: 18.0\n",
            "Step 3011, Episode Reward: 12.0\n",
            "Step 3035, Episode Reward: 24.0\n",
            "Step 3046, Episode Reward: 11.0\n",
            "Step 3061, Episode Reward: 15.0\n",
            "Step 3073, Episode Reward: 12.0\n",
            "Step 3084, Episode Reward: 11.0\n",
            "Step 3118, Episode Reward: 34.0\n",
            "Step 3132, Episode Reward: 14.0\n",
            "Step 3146, Episode Reward: 14.0\n",
            "Step 3156, Episode Reward: 10.0\n",
            "Step 3182, Episode Reward: 26.0\n",
            "Step 3194, Episode Reward: 12.0\n",
            "Step 3212, Episode Reward: 18.0\n",
            "Step 3241, Episode Reward: 29.0\n",
            "Step 3269, Episode Reward: 28.0\n",
            "Step 3293, Episode Reward: 24.0\n",
            "Step 3303, Episode Reward: 10.0\n",
            "Step 3314, Episode Reward: 11.0\n",
            "Step 3327, Episode Reward: 13.0\n",
            "Step 3345, Episode Reward: 18.0\n",
            "Step 3363, Episode Reward: 18.0\n",
            "Step 3377, Episode Reward: 14.0\n",
            "Step 3394, Episode Reward: 17.0\n",
            "Step 3408, Episode Reward: 14.0\n",
            "Step 3425, Episode Reward: 17.0\n",
            "Step 3437, Episode Reward: 12.0\n",
            "Step 3459, Episode Reward: 22.0\n",
            "Step 3468, Episode Reward: 9.0\n",
            "Step 3496, Episode Reward: 28.0\n",
            "Step 3515, Episode Reward: 19.0\n",
            "Step 3534, Episode Reward: 19.0\n",
            "Step 3559, Episode Reward: 25.0\n",
            "Step 3579, Episode Reward: 20.0\n",
            "Step 3593, Episode Reward: 14.0\n",
            "Step 3615, Episode Reward: 22.0\n",
            "Step 3638, Episode Reward: 23.0\n",
            "Step 3684, Episode Reward: 46.0\n",
            "Step 3699, Episode Reward: 15.0\n",
            "Step 3718, Episode Reward: 19.0\n",
            "Step 3736, Episode Reward: 18.0\n",
            "Step 3750, Episode Reward: 14.0\n",
            "Step 3770, Episode Reward: 20.0\n",
            "Step 3785, Episode Reward: 15.0\n",
            "Step 3820, Episode Reward: 35.0\n",
            "Step 3829, Episode Reward: 9.0\n",
            "Step 3844, Episode Reward: 15.0\n",
            "Step 3855, Episode Reward: 11.0\n",
            "Step 3887, Episode Reward: 32.0\n",
            "Step 3902, Episode Reward: 15.0\n",
            "Step 3913, Episode Reward: 11.0\n",
            "Step 3932, Episode Reward: 19.0\n",
            "Step 3941, Episode Reward: 9.0\n",
            "Step 3953, Episode Reward: 12.0\n",
            "Step 3971, Episode Reward: 18.0\n",
            "Step 3986, Episode Reward: 15.0\n",
            "Step 4001, Episode Reward: 15.0\n",
            "Step 4019, Episode Reward: 18.0\n",
            "Step 4034, Episode Reward: 15.0\n",
            "Step 4094, Episode Reward: 60.0\n",
            "Step 4110, Episode Reward: 16.0\n",
            "Step 4149, Episode Reward: 39.0\n",
            "Step 4166, Episode Reward: 17.0\n",
            "Step 4177, Episode Reward: 11.0\n",
            "Step 4192, Episode Reward: 15.0\n",
            "Step 4236, Episode Reward: 44.0\n",
            "Step 4251, Episode Reward: 15.0\n",
            "Step 4263, Episode Reward: 12.0\n",
            "Step 4274, Episode Reward: 11.0\n",
            "Step 4289, Episode Reward: 15.0\n",
            "Step 4301, Episode Reward: 12.0\n",
            "Step 4361, Episode Reward: 60.0\n",
            "Step 4384, Episode Reward: 23.0\n",
            "Step 4396, Episode Reward: 12.0\n",
            "Step 4420, Episode Reward: 24.0\n",
            "Step 4453, Episode Reward: 33.0\n",
            "Step 4484, Episode Reward: 31.0\n",
            "Step 4506, Episode Reward: 22.0\n",
            "Step 4544, Episode Reward: 38.0\n",
            "Step 4575, Episode Reward: 31.0\n",
            "Step 4607, Episode Reward: 32.0\n",
            "Step 4633, Episode Reward: 26.0\n",
            "Step 4703, Episode Reward: 70.0\n",
            "Step 4737, Episode Reward: 34.0\n",
            "Step 4758, Episode Reward: 21.0\n",
            "Step 4794, Episode Reward: 36.0\n",
            "Step 4831, Episode Reward: 37.0\n",
            "Step 4895, Episode Reward: 64.0\n",
            "Step 4914, Episode Reward: 19.0\n",
            "Step 4947, Episode Reward: 33.0\n",
            "Step 4961, Episode Reward: 14.0\n",
            "Step 5001, Episode Reward: 40.0\n",
            "Step 5025, Episode Reward: 24.0\n",
            "Step 5061, Episode Reward: 36.0\n",
            "Step 5120, Episode Reward: 59.0\n",
            "Step 5151, Episode Reward: 31.0\n",
            "Step 5167, Episode Reward: 16.0\n",
            "Step 5215, Episode Reward: 48.0\n",
            "Step 5232, Episode Reward: 17.0\n",
            "Step 5266, Episode Reward: 34.0\n",
            "Step 5275, Episode Reward: 9.0\n",
            "Step 5288, Episode Reward: 13.0\n",
            "Step 5324, Episode Reward: 36.0\n",
            "Step 5356, Episode Reward: 32.0\n",
            "Step 5373, Episode Reward: 17.0\n",
            "Step 5406, Episode Reward: 33.0\n",
            "Step 5475, Episode Reward: 69.0\n",
            "Step 5523, Episode Reward: 48.0\n",
            "Step 5562, Episode Reward: 39.0\n",
            "Step 5632, Episode Reward: 70.0\n",
            "Step 5694, Episode Reward: 62.0\n",
            "Step 5721, Episode Reward: 27.0\n",
            "Step 5755, Episode Reward: 34.0\n",
            "Step 5844, Episode Reward: 89.0\n",
            "Step 5859, Episode Reward: 15.0\n",
            "Step 5878, Episode Reward: 19.0\n",
            "Step 5933, Episode Reward: 55.0\n",
            "Step 5946, Episode Reward: 13.0\n",
            "Step 6014, Episode Reward: 68.0\n",
            "Step 6091, Episode Reward: 77.0\n",
            "Step 6119, Episode Reward: 28.0\n",
            "Step 6173, Episode Reward: 54.0\n",
            "Step 6218, Episode Reward: 45.0\n",
            "Step 6244, Episode Reward: 26.0\n",
            "Step 6316, Episode Reward: 72.0\n",
            "Step 6443, Episode Reward: 127.0\n",
            "Step 6520, Episode Reward: 77.0\n",
            "Step 6645, Episode Reward: 125.0\n",
            "Step 6663, Episode Reward: 18.0\n",
            "Step 6675, Episode Reward: 12.0\n",
            "Step 6712, Episode Reward: 37.0\n",
            "Step 6776, Episode Reward: 64.0\n",
            "Step 6797, Episode Reward: 21.0\n",
            "Step 6927, Episode Reward: 130.0\n",
            "Step 6965, Episode Reward: 38.0\n",
            "Step 6993, Episode Reward: 28.0\n",
            "Step 7021, Episode Reward: 28.0\n",
            "Step 7056, Episode Reward: 35.0\n",
            "Step 7178, Episode Reward: 122.0\n",
            "Step 7259, Episode Reward: 81.0\n",
            "Step 7383, Episode Reward: 124.0\n",
            "Step 7474, Episode Reward: 91.0\n",
            "Step 7509, Episode Reward: 35.0\n",
            "Step 7612, Episode Reward: 103.0\n",
            "Step 7773, Episode Reward: 161.0\n",
            "Step 7944, Episode Reward: 171.0\n",
            "Step 8045, Episode Reward: 101.0\n",
            "Step 8216, Episode Reward: 171.0\n",
            "Step 8365, Episode Reward: 149.0\n",
            "Step 8515, Episode Reward: 150.0\n",
            "Step 8699, Episode Reward: 184.0\n",
            "Step 8871, Episode Reward: 172.0\n",
            "Step 8911, Episode Reward: 40.0\n",
            "Step 9066, Episode Reward: 155.0\n",
            "Step 9247, Episode Reward: 181.0\n",
            "Step 9409, Episode Reward: 162.0\n",
            "Step 9591, Episode Reward: 182.0\n",
            "Step 9749, Episode Reward: 158.0\n",
            "Step 9928, Episode Reward: 179.0\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import math\n",
        "import numpy as np\n",
        "import random\n",
        "import argparse  # for command-line arguments\n",
        "\n",
        "# Try Gymnasium first; fallback to Gym\n",
        "try:\n",
        "    import gymnasium as gym\n",
        "    GYMN = \"gymnasium\"\n",
        "except Exception:\n",
        "    import gym\n",
        "    GYMN = \"gym\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from collections import deque\n",
        "\n",
        "# ---------------------------\n",
        "# Q-Network\n",
        "# ---------------------------\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.fc3 = nn.Linear(128, action_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n",
        "# ---------------------------\n",
        "# Replay Buffer\n",
        "# ---------------------------\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        state, action, reward, next_state, done = map(np.array, zip(*batch))\n",
        "        return state, action, reward, next_state, done\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "# ---------------------------\n",
        "# Training Loop\n",
        "# ---------------------------\n",
        "def train(env_id, total_steps, start_learning, buffer_size, batch_size,\n",
        "          gamma, lr, target_update, eps_start, eps_end, eps_decay_steps):\n",
        "\n",
        "    env = gym.make(env_id)\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.n\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    q_net = QNetwork(state_dim, action_dim).to(device)\n",
        "    target_net = QNetwork(state_dim, action_dim).to(device)\n",
        "    target_net.load_state_dict(q_net.state_dict())\n",
        "    target_net.eval()\n",
        "\n",
        "    optimizer = optim.Adam(q_net.parameters(), lr=lr)\n",
        "    replay_buffer = ReplayBuffer(buffer_size)\n",
        "\n",
        "    epsilon = eps_start\n",
        "    epsilon_decay = (eps_start - eps_end) / eps_decay_steps\n",
        "\n",
        "    state, _ = env.reset()\n",
        "    total_reward = 0\n",
        "\n",
        "    for step in range(1, total_steps + 1):\n",
        "        # Epsilon-greedy action selection\n",
        "        if random.random() < epsilon:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "                q_values = q_net(state_tensor)\n",
        "                action = q_values.argmax().item()\n",
        "\n",
        "        next_state, reward, done, truncated, _ = env.step(action)\n",
        "        replay_buffer.push(state, action, reward, next_state, done or truncated)\n",
        "\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "\n",
        "        # If episode ends\n",
        "        if done or truncated:\n",
        "            print(f\"Step {step}, Episode Reward: {total_reward}\")\n",
        "            state, _ = env.reset()\n",
        "            total_reward = 0\n",
        "\n",
        "        # Training step\n",
        "        if step > start_learning and len(replay_buffer) >= batch_size:\n",
        "            states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
        "\n",
        "            states = torch.FloatTensor(states).to(device)\n",
        "            actions = torch.LongTensor(actions).to(device)\n",
        "            rewards = torch.FloatTensor(rewards).to(device)\n",
        "            next_states = torch.FloatTensor(next_states).to(device)\n",
        "            dones = torch.FloatTensor(dones).to(device)\n",
        "\n",
        "            q_values = q_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
        "            with torch.no_grad():\n",
        "                max_next_q = target_net(next_states).max(1)[0]\n",
        "                target = rewards + gamma * max_next_q * (1 - dones)\n",
        "\n",
        "            loss = F.mse_loss(q_values, target)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Update target network\n",
        "        if step % target_update == 0:\n",
        "            target_net.load_state_dict(q_net.state_dict())\n",
        "\n",
        "        # Decay epsilon\n",
        "        if epsilon > eps_end:\n",
        "            epsilon -= epsilon_decay\n",
        "            epsilon = max(eps_end, epsilon)\n",
        "\n",
        "    env.close()\n",
        "\n",
        "# ---------------------------\n",
        "# Main with argparse\n",
        "# ---------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--env\", type=str, default=\"CartPole-v1\")\n",
        "    parser.add_argument(\"--total-steps\", type=int, default=10000)\n",
        "    parser.add_argument(\"--start-learning\", type=int, default=1000)\n",
        "    parser.add_argument(\"--buffer-size\", type=int, default=10000)\n",
        "    parser.add_argument(\"--batch-size\", type=int, default=32)\n",
        "    parser.add_argument(\"--gamma\", type=float, default=0.99)\n",
        "    parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
        "    parser.add_argument(\"--target-update\", type=int, default=1000)\n",
        "    parser.add_argument(\"--eps-start\", type=float, default=1.0)\n",
        "    parser.add_argument(\"--eps-end\", type=float, default=0.1)\n",
        "    parser.add_argument(\"--eps-decay-steps\", type=int, default=10000)\n",
        "\n",
        "    # ✅ Fix for Jupyter/Colab\n",
        "    args, unknown = parser.parse_known_args()\n",
        "\n",
        "    train(env_id=args.env,\n",
        "          total_steps=args.total_steps,\n",
        "          start_learning=args.start_learning,\n",
        "          buffer_size=args.buffer_size,\n",
        "          batch_size=args.batch_size,\n",
        "          gamma=args.gamma,\n",
        "          lr=args.lr,\n",
        "          target_update=args.target_update,\n",
        "          eps_start=args.eps_start,\n",
        "          eps_end=args.eps_end,\n",
        "          eps_decay_steps=args.eps_decay_steps)\n"
      ]
    }
  ]
}